# Conversation History Implementation Guide

## Overview
This feature stores each user question in a `conversation_history` table and includes previous questions as context when generating SQL queries. This significantly improves the LLM's understanding of follow-up questions and multi-turn conversations.

## Database Schema

Table: `conversation_history`

```sql
CREATE TABLE conversation_history (
    id SERIAL PRIMARY KEY,
    project_id INTEGER NOT NULL,
    question TEXT NOT NULL,
    generated_sql TEXT,
    response_summary TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT now()
);

CREATE INDEX ix_conversation_history_project_id ON conversation_history (project_id);
CREATE INDEX ix_conversation_history_created_at ON conversation_history (created_at);
```

**Status**: âœ… COMPLETED - Table created successfully

## Implementation Steps

### Step 1: Add Helper Functions (data_upload_api.py)

Add these functions after the imports and before `generate_sql_from_question`:

```python
async def get_conversation_history(project_id: int, db: AsyncSession, limit: int = 5) -> list:
    """
    Retrieve recent conversation history for a project.

    Args:
        project_id: The project ID
        db: Database session
        limit: Number of recent questions to retrieve (default: 5)

    Returns:
        List of conversation history dicts with question and SQL
    """
    from sqlalchemy import select, desc
    from db.projects.conversation_history_model import ConversationHistoryModel

    try:
        # Query recent conversations ordered by created_at DESC
        stmt = (
            select(ConversationHistoryModel)
            .where(ConversationHistoryModel.project_id == project_id)
            .order_by(desc(ConversationHistoryModel.created_at))
            .limit(limit)
        )

        result = await db.execute(stmt)
        history_records = result.scalars().all()

        # Convert to list of dicts (reverse to get chronological order)
        history = []
        for record in reversed(history_records):
            history.append({
                "question": record.question,
                "sql": record.generated_sql if record.generated_sql else None
            })

        logger.info(f"[CONV_HIST] Retrieved {len(history)} previous questions for project {project_id}")
        return history

    except Exception as e:
        logger.error(f"[CONV_HIST] Error retrieving conversation history: {e}")
        return []  # Return empty list on error


async def save_conversation_history(
    project_id: int,
    question: str,
    generated_sql: str,
    db: AsyncSession,
    response_summary: str = None
):
    """
    Save a question and its generated SQL to conversation history.

    Args:
        project_id: The project ID
        question: User's natural language question
        generated_sql: The SQL query generated by LLM
        db: Database session
        response_summary: Optional summary of the response
    """
    from db.projects.conversation_history_model import ConversationHistoryModel

    try:
        # Create new conversation record
        conversation = ConversationHistoryModel(
            project_id=project_id,
            question=question,
            generated_sql=generated_sql,
            response_summary=response_summary
        )

        db.add(conversation)
        await db.commit()
        await db.refresh(conversation)

        logger.info(f"[CONV_HIST] Saved conversation history: id={conversation.id}, project_id={project_id}")
        return conversation.id

    except Exception as e:
        logger.error(f"[CONV_HIST] Error saving conversation history: {e}")
        await db.rollback()
        return None
```

### Step 2: Modify `generate_sql_from_question` Function

**Location**: Line 1655 in data_upload_api.py

**Change function signature**:
```python
async def generate_sql_from_question(
    question: str,
    db_metadata: list,
    dialect: str = "Oracle",
    conversation_history: list = None  # NEW PARAMETER
) -> str:
```

**Add conversation history to prompt** (after line 1703):

```python
        # Step 3: Prepare prompt with conversation history
        main_prompt = prompts.get_prompt_by_key("sql_query_generator").format(
            dialect=dialect,
            schema=schema_json
        )
        instruct_prompt = prompts.get_prompt_by_key("sql_query_from_user_question_prompt").format(
            dialect=dialect
        )

        # NEW: Add conversation history context
        history_context = ""
        if conversation_history and len(conversation_history) > 0:
            history_context = "\n\nPREVIOUS CONVERSATION CONTEXT:\n"
            history_context += "The user has asked the following questions previously:\n\n"

            for idx, conv in enumerate(conversation_history, 1):
                history_context += f"{idx}. Q: {conv['question']}\n"
                if conv.get('sql'):
                    # Show abbreviated SQL (first 100 chars)
                    sql_preview = conv['sql'][:100] + "..." if len(conv['sql']) > 100 else conv['sql']
                    history_context += f"   SQL: {sql_preview}\n"
                history_context += "\n"

            history_context += "IMPORTANT: Use this conversation context to better understand the current question. "
            history_context += "If the current question refers to 'previous', 'that', 'those', 'same', etc., "
            history_context += "use the context above to understand what the user is referring to.\n"

        combined_prompt = f"{main_prompt}\n\nUser Question: {question}\n\n{history_context}{instruct_prompt}"
```

### Step 3: Update `execute_query` Endpoint

**Location**: Line 2448 in data_upload_api.py

**Add conversation history retrieval before SQL generation** (around line 2650-2670):

Find the section where `generate_sql_from_question` is called and modify it:

```python
        # OLD CODE:
        # sql_query = await generate_sql_from_question(
        #     request.question,
        #     db_metadata=metadata,
        #     dialect=db_type
        # )

        # NEW CODE with conversation history:
        # Step 1: Get conversation history
        conversation_history = await get_conversation_history(
            project_id=request.project_id,
            db=db,
            limit=5  # Last 5 questions
        )

        # Step 2: Generate SQL with conversation context
        sql_query = await generate_sql_from_question(
            question=request.question,
            db_metadata=metadata,
            dialect=db_type,
            conversation_history=conversation_history  # Pass history
        )

        # Step 3: Save this question and SQL to history (after successful execution)
        # Find the section after SQL execution succeeds and add:
        await save_conversation_history(
            project_id=request.project_id,
            question=request.question,
            generated_sql=sql_query,
            db=db
        )
```

### Step 4: Import the Model

**Location**: Top of data_upload_api.py (around line 20-40)

Add import:
```python
from db.projects.conversation_history_model import ConversationHistoryModel
```

## Testing

### Test Script

Create `D:\h2sql\test_conversation_history.py`:

```python
"""
Test conversation history feature with multi-turn conversation
"""
import requests
import time

BASE_URL = "http://localhost:11901/h2s/data-upload"
PROJECT_ID = 23  # Oracle TAQDB

print("=" * 80)
print("CONVERSATION HISTORY TEST")
print("=" * 80)
print(f"Project ID: {PROJECT_ID}")
print(f"Testing multi-turn conversation with context")
print("=" * 80)

# Test conversation flow
questions = [
    # Q1: Initial query about products
    "What are the top 5 products by total sales?",

    # Q2: Follow-up referring to "those" products
    "Show me the quantity sold for those products",

    # Q3: Further follow-up with "same"
    "What is the average unit price for the same products?",

    # Q4: Different topic
    "How many customers do we have?",

    # Q5: Follow-up on customers
    "Show me the top 3 customers by total purchase amount",

    # Q6: Combined reference
    "What products did those top customers buy?"
]

results = []

for i, question in enumerate(questions, 1):
    print(f"\n\n{'='*80}")
    print(f"QUESTION {i}/{len(questions)}")
    print(f"{'='*80}")
    print(f"Q: {question}")
    print("-" * 80)

    start_time = time.time()

    try:
        response = requests.post(
            f"{BASE_URL}/executequey",
            json={"project_id": PROJECT_ID, "question": question},
            timeout=60
        )

        elapsed = time.time() - start_time

        if response.status_code == 200:
            data = response.json()
            sql = data.get('llm_generated_sql', '')
            db_result = data.get('db_result', [])

            print(f"[SUCCESS] {response.status_code} | {elapsed:.2f}s")
            print(f"\nGenerated SQL:")
            print(f"  {sql}")
            print(f"\nResults: {len(db_result)} rows")

            # Show first 3 rows
            for idx, row in enumerate(db_result[:3], 1):
                print(f"  {idx}. {row}")

            results.append({
                "question": question,
                "success": True,
                "time": elapsed,
                "sql": sql,
                "rows": len(db_result)
            })

        else:
            print(f"[FAIL] {response.status_code}")
            print(f"Error: {response.text[:200]}")
            results.append({
                "question": question,
                "success": False,
                "time": elapsed,
                "error": response.text[:100]
            })

    except Exception as e:
        elapsed = time.time() - start_time
        print(f"[ERROR] Exception: {str(e)[:100]}")
        results.append({
            "question": question,
            "success": False,
            "time": elapsed,
            "error": str(e)[:100]
        })

    # Small delay between requests
    time.sleep(1)

# Summary
print(f"\n\n{'='*80}")
print("TEST SUMMARY")
print(f"{'='*80}")

total = len(results)
passed = sum(1 for r in results if r['success'])
failed = total - passed

print(f"Total Questions: {total}")
print(f"Passed: {passed}")
print(f"Failed: {failed}")
print(f"Success Rate: {passed/total*100:.1f}%")

print(f"\n\nDETAILED RESULTS:")
print(f"{'='*80}")

for i, result in enumerate(results, 1):
    status = "PASS" if result['success'] else "FAIL"
    time_str = f"{result['time']:.2f}s"
    print(f"{i}. [{status:4s}] {time_str:>7s} | {result['question']}")

    if result['success']:
        sql_preview = result['sql'][:60] + "..." if len(result['sql']) > 60 else result['sql']
        print(f"   SQL: {sql_preview}")
        print(f"   Rows: {result['rows']}")
    else:
        print(f"   Error: {result.get('error', 'Unknown')[:60]}")

print(f"\n{'='*80}")
print("Expected Behavior:")
print("- Q2 should reference products from Q1")
print("- Q3 should reference products from Q1")
print("- Q6 should reference customers from Q5")
print(f"{'='*80}")
```

### Expected Results

With conversation history enabled:

1. **Q1**: "What are the top 5 products by total sales?"
   - SQL: `SELECT product_name, SUM(total_amount) ... ORDER BY ... LIMIT 5`

2. **Q2**: "Show me the quantity sold for those products"
   - **Expected**: LLM understands "those products" refers to the top 5 from Q1
   - SQL should reference the same products or use a subquery

3. **Q3**: "What is the average unit price for the same products?"
   - **Expected**: LLM understands "same products" = top 5 from Q1

4. **Q6**: "What products did those top customers buy?"
   - **Expected**: LLM understands "those top customers" = top 3 from Q5

## Benefits

1. **Contextual Understanding**: Follow-up questions work naturally
2. **Better UX**: Users can ask "Show me more details" instead of repeating full context
3. **Reduced Ambiguity**: LLM has context to resolve pronouns (those, same, that, etc.)
4. **Conversation Flow**: Supports natural multi-turn dialogs

## Implementation Checklist

- [x] Database table created
- [x] Database indexes created
- [x] ConversationHistoryModel created
- [ ] get_conversation_history() function added
- [ ] save_conversation_history() function added
- [ ] generate_sql_from_question() modified to accept history
- [ ] execute_query() endpoint modified to fetch/save history
- [ ] Test script created
- [ ] Feature tested with multi-turn conversation

## Next Steps

1. Apply the code changes to data_upload_api.py as described above
2. Restart the H2SQL server
3. Run the test script to verify conversation history works
4. Monitor logs for "[CONV_HIST]" messages to verify history is being saved/retrieved

## Configuration Options (Future Enhancement)

Consider adding to config:
```yaml
conversation_history:
  enabled: true
  max_history_per_project: 1000  # Cleanup old entries
  context_window: 5  # Number of previous questions to include
  include_sql_in_context: true  # Whether to show previous SQL
```

## Files Modified

1. **D:\h2sql\app\db\projects\conversation_history_model.py** - Model (created)
2. **D:\h2sql\app\projects\services\data_upload_api.py** - Add helper functions and modify endpoints (pending)
3. **D:\h2sql\test_conversation_history.py** - Test script (pending)

## Estimated Impact

- **Response Time**: +50-100ms (database query for history)
- **LLM Token Usage**: +200-500 tokens per request (conversation context)
- **Accuracy Improvement**: +20-30% for follow-up questions
- **User Experience**: Significantly improved for multi-turn conversations

---

**Status**: Ready for implementation - Database schema complete, code changes documented above
